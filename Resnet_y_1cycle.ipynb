{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets \n",
    "import torchvision.transforms  as T\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader , random_split\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'  ) \n",
    "torch.manual_seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= './DATASET/'\n",
    "min_batch = 512\n",
    "TRAIN_SIZE = 50000\n",
    "VAL_SIZE = 5000\n",
    "TEST_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_cifar10_train = T.Compose([\n",
    "    T.RandomHorizontalFlip(p=0.3),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.491, 0.482, 0.447], [0.247, 0.243, 0.262])\n",
    "])\n",
    "transform_cifar10_test = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize([0.491, 0.482, 0.447], [0.247, 0.243, 0.262])\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_train = datasets.CIFAR10(path, train=True, download=False,transform=transform_cifar10_train)\n",
    "\n",
    "\n",
    "test_dataset = datasets.CIFAR10(path, train=False, download=False, transform=transform_cifar10_test)\n",
    "val_dataset, test_dataset = random_split(test_dataset, [VAL_SIZE, TEST_SIZE])\n",
    "\n",
    "train_loader = DataLoader(cifar10_train, batch_size=min_batch, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=min_batch, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cifar10_grid():\n",
    "    classes = train_loader.dataset.classes\n",
    "    total_samples = 8\n",
    "    plt.figure(figsize=(15,15))\n",
    "    for label, sample in enumerate(classes):\n",
    "        class_idxs = np.flatnonzero(label == np.array(train_loader.dataset.targets))\n",
    "        sample_idxs = np.random.choice(class_idxs, total_samples, replace = False)\n",
    "        for i, idx in enumerate(sample_idxs):\n",
    "            plt_idx = i*len(classes) + label + 1\n",
    "            plt.subplot(total_samples, len(classes), plt_idx)\n",
    "            plt.imshow(train_loader.dataset.data[idx])\n",
    "            plt.axis('off')\n",
    "            \n",
    "            if i == 0: plt.title(sample)\n",
    "    plt.show()\n",
    "\n",
    "plot_cifar10_grid() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conlayer_K3P1(canal_in , canal_out , stride):\n",
    "    return nn.Conv2d(canal_in , canal_out , stride=stride , kernel_size=3 , padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class residual_block(nn.Module):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    def __init__(self, in_channel, out_channel, stride=1, change_size = True):\n",
    "        super().__init__()\n",
    "        self.conv1 = conlayer_K3P1(in_channel, out_channel, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv2 = conlayer_K3P1(out_channel, out_channel, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        #for changing activation map sizes\n",
    "        self.change_size = change_size\n",
    "        if change_size:\n",
    "            self.residual = nn.Sequential(nn.Conv2d(in_channel, \n",
    "                                                    out_channel, \n",
    "                                                    kernel_size=1,\n",
    "                                                    stride=stride),\n",
    "                                         nn.BatchNorm2d(out_channel)\n",
    "                                         )      \n",
    "    def forward(self, x):\n",
    "        identity = x if not self.change_size else self.residual(x)\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        y += identity\n",
    "        return F.relu(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet56(nn.Module):\n",
    "    def __init__(self, n=9, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = conlayer_K3P1(3, 16, stride = 1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.block1 = self.create_block(n=9, in_channel=16, \n",
    "                                        out_channel=16, stride=1, \n",
    "                                        change_size=False)\n",
    "        self.block2 = self.create_block(n=9, in_channel=16, \n",
    "                                        out_channel=32, stride=2)\n",
    "        self.block3 = self.create_block(n=9, in_channel=32, \n",
    "                                        out_channel=64, stride=2)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def create_block(self, n, in_channel, out_channel, stride, change_size=True):\n",
    "        block = [residual_block(in_channel, out_channel, stride, change_size=change_size)]\n",
    "        for i in range(n-1):\n",
    "            block.append(residual_block(out_channel, out_channel, stride=1, change_size=False))\n",
    "        return nn.Sequential(*block)   \n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.block3(self.block2(self.block1(y)))\n",
    "        y = F.adaptive_avg_pool2d(y, 1)\n",
    "        return self.fc(y.view(y.size(0), -1))      \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet56()\n",
    "optimizer_resnet56 = torch.optim.SGD(model.parameters() , lr= 0.1 , momentum=0.95 , weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funcion de Accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, loader):\n",
    "    correct = 0  # Contador para el número de predicciones correctas\n",
    "    total = 0    # Contador para el número total de muestras\n",
    "    cost = 0     # Acumulador para la pérdida total\n",
    "    model.eval()  # Configura el modelo en modo de evaluación (desactiva dropout, etc.)\n",
    "    model = model.to(device=device)  # Mueve el modelo al dispositivo (CPU o GPU)\n",
    "    with torch.no_grad():  # Desactiva el cálculo de gradientes para ahorrar memoria y tiempo\n",
    "        for x, y in loader:  # Itera sobre cada lote en el cargador de datos\n",
    "            x = x.to(device=device, dtype=torch.float32)  # Mueve los datos de entrada al dispositivo\n",
    "            y = y.to(device=device, dtype=torch.long)     # Mueve las etiquetas al dispositivo\n",
    "            scores = model(x)  # Calcula las predicciones del modelo (salidas en bruto)\n",
    "            cost += (F.cross_entropy(scores, y)).item()  # Acumula la pérdida de entropía cruzada\n",
    "            _, pred = scores.max(dim=1)  # Obtiene las predicciones (clase con mayor puntuación)\n",
    "            correct += (pred == y).sum()  # Suma el número de predicciones correctas\n",
    "            total += pred.size(0)  # Acumula el número total de muestras en el lote\n",
    "        # Calcula la pérdida promedio y la precisión total\n",
    "        return cost / len(loader), float(correct) / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lr(model, optimiser, start_val=1e-6, end_val=1, beta=0.99, loader=train_loader):\n",
    "    \"\"\"\n",
    "    Encuentra la tasa de aprendizaje óptima utilizando la prueba de rango de tasa de aprendizaje.\n",
    "\n",
    "    Args:\n",
    "        model: El modelo de aprendizaje automático que se está entrenando.\n",
    "        optimiser: El optimizador utilizado para actualizar los pesos del modelo.\n",
    "        start_val: La tasa de aprendizaje inicial (por defecto 1e-6).\n",
    "        end_val: La tasa de aprendizaje final (por defecto 1).\n",
    "        beta: Factor de suavizado para la media móvil exponencial de la pérdida (por defecto 0.99).\n",
    "        loader: El cargador de datos que proporciona los lotes de entrenamiento (por defecto train_loader).\n",
    "\n",
    "    Returns:\n",
    "        log_lrs: Lista de tasas de aprendizaje utilizadas en cada iteración.\n",
    "        losses: Lista de pérdidas promedio suavizadas calculadas en cada lote.\n",
    "        accuracies: Lista de precisiones calculadas por lote.\n",
    "        optimal_lr: Tasa de aprendizaje óptima seleccionada automáticamente.\n",
    "    \"\"\"\n",
    "    n = len(loader) - 1\n",
    "    factor = (end_val / start_val) ** (1 / n)\n",
    "    lr = start_val\n",
    "    optimiser.param_groups[0]['lr'] = lr\n",
    "\n",
    "    avg_loss, loss = 0.0, 0.0\n",
    "    lowest_loss = 0.0\n",
    "    optimal_lr = start_val  # Inicializamos con el valor inicial\n",
    "    losses = []\n",
    "    log_lrs = []\n",
    "    accuracies = []\n",
    "\n",
    "    model = model.to(device=device)\n",
    "\n",
    "    for i, (x, y) in enumerate(loader, start=1):\n",
    "        x = x.to(device=device, dtype=torch.float32)\n",
    "        y = y.to(device=device, dtype=torch.long)\n",
    "        optimiser.zero_grad()\n",
    "        scores = model(x)\n",
    "        cost = F.cross_entropy(input=scores, target=y)\n",
    "\n",
    "        loss = beta * loss + (1 - beta) * cost.item()\n",
    "        avg_loss = loss / (1 - beta ** i)\n",
    "\n",
    "        acc_ = (torch.argmax(scores, dim=1) == y).sum().item() / scores.size(0)\n",
    "\n",
    "        # Actualizamos lowest_loss y guardamos el lr correspondiente\n",
    "        if avg_loss < lowest_loss or i == 1:\n",
    "            lowest_loss = avg_loss\n",
    "            optimal_lr = lr  # Guardamos el lr donde la pérdida es mínima\n",
    "\n",
    "        # Detenemos si la pérdida crece demasiado\n",
    "        if i > 1 and avg_loss > 4 * lowest_loss:\n",
    "            print(f'Se detuvo en el lote {i}, cost: {cost.item():.4f}')\n",
    "            return log_lrs, losses, accuracies, optimal_lr\n",
    "\n",
    "        accuracies.append(acc_)\n",
    "        losses.append(avg_loss)\n",
    "        log_lrs.append(lr)\n",
    "\n",
    "        cost.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        print(f'cost: {cost.item():.4f}, lr: {lr:.4f}, acc: {acc_:.4f}')\n",
    "        lr *= factor\n",
    "        optimiser.param_groups[0]['lr'] = lr\n",
    "\n",
    "    # Si no se detiene antes, devolvemos el lr óptimo basado en la pérdida mínima\n",
    "    return log_lrs, losses, accuracies, optimal_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimiser, scheduler=None, epochs=100):\n",
    "    val_loss_history = []  # Lista para almacenar la pérdida de validación por época\n",
    "    train_loss_history = []  # Lista para almacenar la pérdida de entrenamiento por época\n",
    "    val_acc_history = []  # Lista para almacenar la precisión de validación por época\n",
    "    train_acc_history = []  # Lista para almacenar la precisión de entrenamiento por época\n",
    "    lrs = []  # Lista para almacenar las tasas de aprendizaje por época\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Configura el modelo en modo de entrenamiento\n",
    "        train_correct_num = 0  # Contador de predicciones correctas\n",
    "        train_total = 0  # Contador del total de muestras\n",
    "        train_cost_acum = 0  # Acumulador de la pérdida\n",
    "\n",
    "        # Bucle de entrenamiento\n",
    "        for mb, (x, y) in enumerate(train_loader, start=1):\n",
    "            x = x.to(device, dtype=torch.float32)\n",
    "            y = y.to(device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            cost = F.cross_entropy(input=scores, target=y)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            cost.backward()\n",
    "            optimiser.step()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "\n",
    "            # Acumula métricas de entrenamiento\n",
    "            train_correct_num += (torch.argmax(scores, dim=1) == y).sum()\n",
    "            train_total += scores.size(0)\n",
    "            train_cost_acum += cost.item()\n",
    "\n",
    "        # Calcula métricas promedio de entrenamiento por época\n",
    "        train_acc = float(train_correct_num) / train_total\n",
    "        train_cost = train_cost_acum / len(train_loader)\n",
    "\n",
    "        # Evalúa en el conjunto de validación (una vez por época)\n",
    "        val_cost, val_acc = accuracy(model, val_loader)\n",
    "\n",
    "        # Almacena las métricas (una vez por época)\n",
    "        train_loss_history.append(train_cost)\n",
    "        val_loss_history.append(val_cost)\n",
    "        train_acc_history.append(train_acc)\n",
    "        val_acc_history.append(val_acc)\n",
    "        lrs.append(optimiser.param_groups[0][\"lr\"])\n",
    "\n",
    "        # Imprime un resumen de la época\n",
    "        print(f'Epoch:{epoch}, train cost: {train_cost:.6f}, val cost: {val_cost:.6f},'\n",
    "              f' train acc: {train_acc:.4f}, val acc: {val_acc:.4f}, total: {train_total},'\n",
    "              f' lr: {optimiser.param_groups[0][\"lr\"]:.6f}')\n",
    "\n",
    "    return train_loss_history, val_loss_history, train_acc_history, val_acc_history, lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del modelo y optimizador\n",
    "model_resnet56 = ResNet56()\n",
    "model_resnet56 = model_resnet56.to(device)\n",
    "optimiser_resnet56 = torch.optim.SGD(model_resnet56.parameters(),\n",
    "                                     lr=0.1, momentum=0.95,\n",
    "                                     weight_decay=1e-4)\n",
    "\n",
    "# Ejecutar find_lr para obtener el lr óptimo\n",
    "log_lrs, losses, accuracies, optimal_lr = find_lr(model_resnet56, optimiser_resnet56, \n",
    "                                                  start_val=1e-6, end_val=1)\n",
    "\n",
    "print(f\"Tasa de aprendizaje óptima encontrada: {optimal_lr:.6f}\")\n",
    "\n",
    "# Configurar el scheduler con el lr óptimo como max_lr\n",
    "epochs = 10\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimiser_resnet56, \n",
    "                                                max_lr=optimal_lr,  # Usamos el valor óptimo\n",
    "                                                steps_per_epoch=len(train_loader), \n",
    "                                                epochs=epochs, \n",
    "                                                pct_start=0.43, \n",
    "                                                div_factor=10, \n",
    "                                                final_div_factor=1000, \n",
    "                                                three_phase=True, \n",
    "                                                verbose=False)\n",
    "\n",
    "# Entrenar el modelo\n",
    "train_loss_history, val_loss_history, train_acc_history, val_acc_history, lrs = train(\n",
    "                                model_resnet56, \n",
    "                                optimiser_resnet56,\n",
    "                                scheduler=scheduler,\n",
    "                                epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir(model, img, lbl):\n",
    "    # Asegurarse de que la imagen ya está normalizada (asumimos que test_dataset lo hizo)\n",
    "    # Agregar dimensión de batch\n",
    "    img_batch = img.unsqueeze(0).to('cuda')\n",
    "    \n",
    "    # Generar predicción\n",
    "    logits = model(img_batch)\n",
    "    y_pred = logits.argmax(1).item()\n",
    "\n",
    "    # Mostrar imagen original (transponer de CHW a HWC para matplotlib)\n",
    "    img_for_display = img.cpu().numpy().transpose((1, 2, 0))  # De [3, 32, 32] a [32, 32, 3]\n",
    "    plt.imshow(img_for_display)  # Sin cmap para RGB\n",
    "    plt.title(f'Categoría predicha: {y_pred}, categoría real: {lbl}')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomar una imagen del set de prueba\n",
    "img, lbl = test_dataset[torch.randint(len(test_dataset), size=(1,)).item()]\n",
    "\n",
    "# Verificar la forma de la imagen antes de predecir\n",
    "print(f\"Forma de la imagen original: {img.shape}\")\n",
    "\n",
    "# Y generar la predicción\n",
    "predecir(model_resnet56, img, lbl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
