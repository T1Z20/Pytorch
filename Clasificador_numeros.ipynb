{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practica : entrenar red nbeuronal para clasifiar imagenes de numeros (28x28) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "## Un tensor es una manera organizada de almacenar un arreglo de datos (EJ : datos tabulares , imagenes ,audio , texto ) puede ser tanto un vector (1 dimension) como una matriz (2 dimensiones) o mas dimensiones para datos mas complejos (imagenes a color , videos )\n",
    "\n",
    "\n",
    "## Ejemplo \n",
    "\n",
    "Arreglo_datos = [[1,2,3],[4,5,6]]\n",
    "Tensor_dedatos = torch.tensor(Arreglo_datos)\n",
    "\n",
    "print(type(Tensor_dedatos))\n",
    "print(Tensor_dedatos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Otra caracteristica es que estos datos pueden ser procesados tanto por gpu como cpu (torch.cuda.is_available())\n",
    "\n",
    "Tensor_dedatos.device #.device para saber donde se esta almacenando\n",
    "\n",
    "##Es posible cambiar el lugar de procesamiento con .to(cpu/cuda)\n",
    "#Tensor_dedatos.to(\"cuda\").device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Al ser un arreglo de datos un Tensor tiene atributos \n",
    "Tensor_dedatos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA SET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## En pytorch existen 2 modulos que nos permitesn  cagar sets de datos \n",
    "\n",
    "torch.utils.data.DataLoader # Nos permite crear los bachs para entrenar el modelo (iterar sobre el data set )\n",
    "\n",
    "torch.utils.data.Dataset # Nos permite leer datos que esten almacenados en X lugar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En caso de estar trabajando con imagenes usaremos \n",
    "import torchvision\n",
    "\n",
    "torchvision.dataset # El cual nos permite cargar el set de datos con imagenes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora Conociendo estos 2 conceptos (data set y tensor) podemos pasar a la practica "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero importamos las librerias necesarias \n",
    "\n",
    "from torchvision import datasets # descargar el data set (este se encuentra en torch)\n",
    "from torchvision.transforms import ToTensor # trabajar con tensores \n",
    "import matplotlib.pyplot as plt # hacer graficas \n",
    "\n",
    "dataset_28x28 = datasets.MNIST (\n",
    "    \n",
    "    root='./DATASET/', # donde queremos descargar \n",
    "    train=True, # descargar la totalidad de imagenes (cuando trabajamos con un data et para deep learning suele estar dividido en train and validation)\n",
    "    download=True,\n",
    "    transform=ToTensor() # Transforma las imagenes a tensores para no tener que hacerlo luego \n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./DATASET/\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_28x28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observamos el algunos ejemplos del set de datos \n",
    "\n",
    "plot = plt.figure(figsize=(8,8))\n",
    "fila , columna = 3 ,3 \n",
    "\n",
    "for i in range(1 , columna * fila + 1 ):\n",
    "    pick = torch.randint(len(dataset_28x28), size=(1,)).item()\n",
    "    \n",
    "    imgaen , label = dataset_28x28[pick]\n",
    "    \n",
    "    plot.add_subplot(fila,columna,i)\n",
    "    plt.title(str(label))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(imgaen.squeeze() , cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como mencione antes importamos unas 60 mil imganes pero estan no estan particionadas . \n",
    "    siempre que trabajemos en una red nueronal deberemos realizar la partidicion de datos (Entrenamiento , validacion y prueba)\n",
    "    Nornalmente la particion mas grande es la de entrenamiento donde 80% (segun la cantidad de datos que tengamos)\n",
    "    luego tenemos la particion de validacion que se usa a la par que la de entrenamiento para validar que tan bien esta respondiendo al entrenamiento \n",
    "    y fginalmente con la de prueba probamos el modelo ya entrenado con datos que el modelo nunca vio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b832b38690>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(69) #Es recomendable fijar una semilla de aleatoridad para que al volver a correr el codigo tengamos resultados similares "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000\n",
      "6000\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "##Para deividir el set de entrenamiento usaremos usaremos Prueba , validacion , entrenamiento = random_split() dentro de ( pondremos el data set y los procentajes  )\n",
    "\n",
    "\n",
    "entrenamiento , validacion , prueba = torch.utils.data.random_split( dataset_28x28 , [0.8 , 0.1 , 0.1] )\n",
    "\n",
    "\n",
    "print(len(entrenamiento))\n",
    "print(len(validacion))\n",
    "print(len(prueba))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora con esto podremos empezar con la 'arquitectura' de nuestro modelo \n",
    "\n",
    "recordamnos que las redenes neruonales se deividen en \n",
    "\n",
    "capas de entrada = cantidad de variables que entren , Por ejemplo en este caso entran 28*28 pixeles de entrada = 784 variables aunque deberemos aplanarla entrada \n",
    "\n",
    "capas ocultas = capas donde ocurre el forward propagation\n",
    "\n",
    "capas de salida = segun la cantidad de categorias o necesidades a predecir (10 digitos = 10 neuronas) y usamos softmax para transformar el valor de salida en una probabilidad "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para crear las redes neuronales en torch usamos nn.Module usando POO es decir nuestras redes neuronales van a ser subclases de nn.Module\n",
    "## Nuestra red neuronal siempre tendra 2 metodos dentro de la subclase \n",
    "    INIT = el cual va a definir su arquitectura (cantidad de neuronas , capas etc) \n",
    "    forward = el cual va a definir como fluyen los valores por la red neuronal (la conexion entre las neuronas) y como se hace cada prediccion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn \n",
    "\n",
    "class Redneuronal(nn.Module): ## al escribir nn.module estamos diciendo que la clase redneuronal es hija de module\n",
    "    ## estas 2 lineas de abajo siempre iran ya que son estandar de torch para hacer funcionar la red y inicializar el modelo\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        #Ahora si modemos crear la arquitectura \n",
    "        \n",
    "        self.aplanar = nn.Flatten() # transforma el tensor de (1x28x28) en un vector de (1 , 784)\n",
    "        self.red = nn.Sequential( #Esto en torch es una forma de defininir un patron que indica una secuencia de capas aplicadas en el orden dado\n",
    "            \n",
    "            nn.Linear(28*28 , 15), ##Primero se define la capa de entrada + con cuantas neuronas se va a conectar (capa oculta)\n",
    "            nn.ReLU(), ##Aunque arriba hayamos definido la cantidad de neuronas de la capa oculta nos falta definir su funcion de activacion\n",
    "            nn.Linear(15,10), ## Decimos la capa de salida conectada con las 15 neuronas de la capa oculta \n",
    "            nn.Softmax() ##Esto en realidad no es necesario ya que lo unico que hace es transformar los 10 valores de salida en numeros cuya suma sea 1 (probabilidades)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self , x ): # X hace referencia al dato de entrada y la secuancia que va a tomar\n",
    "        x = self.aplanar(x)\n",
    "        probabilidades = self.red(x)\n",
    "\n",
    "        return probabilidades\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora con la arquitectura podemos crear una instacian de la clase y moverla a la gpu / cpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Redneuronal(\n",
       "  (aplanar): Flatten(start_dim=1, end_dim=-1)\n",
       "  (red): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=15, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=15, out_features=10, bias=True)\n",
       "    (3): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Modelo = Redneuronal().to('cuda')\n",
    "Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso de querer saber la cantidad de pramatros para un modelo podemos uar el metodo parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11935\n"
     ]
    }
   ],
   "source": [
    "Parametros = sum(p.numel() for p in Modelo.parameters()) #p.numel() accede a la cantidad de perametros por capa si itera y se suma\n",
    "print(Parametros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el modelo ya listos podemos pasar a entrenar al modelo pero antes dejaremos los sets de entrenamiento y validacion listos \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Creamos 2 data loaders uno para el entrenamiento y otro para la validacion (como dije antes estos se usan al mismo tiempo para corroborar que el entrenamiento este yendo bien)\n",
    "#Shuffle mezcla los datos \n",
    "\n",
    "train_loader = DataLoader( \n",
    "    dataset=entrenamiento,                          \n",
    "    batch_size=1000,                      \n",
    "    shuffle=True                       \n",
    "                          )\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=validacion,\n",
    "    batch_size=1000,\n",
    "    shuffle=True  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora pdoemos preparar los hiperparametros para iniciar el entrenamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1 \n",
    "Epochs = 10 \n",
    "fun_perdida = nn.CrossEntropyLoss()\n",
    "optimizador = torch.optim.SGD(Modelo.parameters() , lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora a entrenar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenamiento_loop (data_loader , modelo , fun_perdida , optimizador ): \n",
    "    #cantidad de datos y cantidad de lotes \n",
    "    train_size = len ( data_loader.dataset)\n",
    "    nlotes = len ( data_loader)\n",
    "    #pondremos el modelo en modo entrenamiento \n",
    "    modelo.train()\n",
    "    \n",
    "    #Ahora necesitamos  calcular 2 cosas perdida y exactitud (por cuanto se equivoca y que  tanto)\n",
    "    perdida , accuracy = 0 ,0 \n",
    "    \n",
    "    for nlotes , (x,y) in enumerate(data_loader):\n",
    "        x , y = x.to(\"cuda\") , y.to('cuda')\n",
    "        #forward\n",
    "        probabilidad = modelo(x)\n",
    "        #backward\n",
    "        loss = fun_perdida(probabilidad , y)\n",
    "        loss.backward()\n",
    "        optimizador.step()\n",
    "        optimizador.zero_grad()\n",
    "    \n",
    "    \n",
    "        perdida += loss.item()\n",
    "        accuracy += (probabilidad.argmax(1)==y).type(torch.float).sum().item()\n",
    "        \n",
    "        \n",
    "        if nlotes % 10 : \n",
    "            loss ,ndatos = loss.item() , nlotes*train_size\n",
    "    \n",
    "        print(f'\\tPerdida: {loss.item():>7f} [{ndatos:>5d}/{train_size:>5d}]')\n",
    "    \n",
    "    perdida /= nlotes\n",
    "    accuracy /= train_size\n",
    "    print(f'\\t Exactitud/perdida promedio:')\n",
    "    print(f'\\t\\t entrenamiento: {(100*accuracy):>0.1f}% / {perdida:>8f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
